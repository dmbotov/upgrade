{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Туториал по анализу тональности твитов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Знакомимся с корпусом твитов\n",
    "\n",
    "Возьмем за основу корпус русскоязычных твитов RuTweetCorp http://study.mokoron.com/\n",
    "\n",
    "![RuTweetCorp](http://study.mokoron.com/wp-content/uploads/2014/01/bd1.png)\n",
    "\n",
    "В корпусе есть разметка по тональности твитов на позитивные и негативные\n",
    "\n",
    "[Данные для обучения](https://drive.google.com/open?id=14uzgtFWkS6-HVWq6wClYVvlcUo2L8JgJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget http://vectors.nlpl.eu/repository/20/185.zip\n",
    "\n",
    "!mv ./185.zip data/model_185.zip\n",
    "!unzip data/data-20200219T170811Z-001.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymystem3, gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем 5000 позитивных и негативных твитов\n",
    "\n",
    "df_tweets = pd.read_csv('./data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: сохраните в отдельный датафрейм по 5 примеров позитивных и негативных твитов (всего будет 10 твитов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка текстов\n",
    "\n",
    "1. **Токенизация** (англ. *tokenization*) — разбиение текста на **токены**: отдельные фразы, слова, символы. \n",
    "2. **Лемматизация** (англ. *lemmatization*) — приведение слова к начальной форме (**лемме**).\n",
    "3. **Удаление стоп-слов** — удаление частотных слов, в которых нет ценной семантики - местоимений, союзов, предлогов и т.д.\n",
    "\n",
    "Функция лемматизации русского текста есть в библиотеках: \n",
    "\n",
    "- *pymorphy2* (англ. *python morphology,* «морфология для Python»), **\n",
    "- *UDPipe* (англ. *universal dependencies pipeline*, «конвейер для построения общих зависимостей»*),*\n",
    "- *pymystem3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем метод очистки текста с удаление прочих символов кроме кириллицы\n",
    "\n",
    "def clear_text(text):\n",
    "    clean_text = re.sub(r'[^а-яА-ЯёЁ]',' ',text)\n",
    "    clean_text = \" \".join(clean_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lemmatize(\"лемматизируй это\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: реализуйте метод лемматизации, который возвращает строку лемматизированного текста \n",
    "#  (используйте ' '.join для склейки массива токенов в строку)\n",
    "\n",
    "def lemmatize(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: сделайте очистку текста и лемматизируйте выборку из 10 твитов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: добавьте лемматизированный текст в виде нового столбца в датафрейм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация через mystem долгий процесс, загрузим корпус с лемматизированными твитами\n",
    "\n",
    "df_tweets = pd.read_csv(\".\\\\data\\\\tweets_lemm.csv\")\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: удалите дубликаты твитов из корпуса (а они там есть!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_tweets['lemm_text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для работы со стоп-словами используем готовые списки из библиотеки nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, что считается стоп-словами\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов\n",
    "\n",
    "\n",
    "Преобразовать слова в векторы поможет модель **«мешок слов»** (англ. *bag of words*). Она преобразует текст в вектор, не учитывая порядок слов. Отсюда и название — «мешок».\n",
    "\n",
    "**Пример корпуса после лемматизации:**\n",
    "[\n",
    "\t'ехать Гpека чеpез pека',\n",
    "\t'видеть Гpека в pека pак', \n",
    "\t'сунуть Гpека pука в pека',\n",
    "\t'pак за pука Гpека цап'\n",
    "]\n",
    "\n",
    "**Мешок слов:**\n",
    "['ехать', 'Гpека', 'чеpез', 'pека', 'видеть', 'в', 'pак', 'сунуть', 'pука', 'за', 'цап']\n",
    "\n",
    "**Задание**\n",
    "Какой вектор описывает лемматизированный текст **«видеть Гpека в pека pак»** ?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_greka = [\n",
    "    'ехать Гpека чеpез pека',\n",
    "    'видеть Гpека в pека pак', \n",
    "    'сунуть Гpека pука в pека',\n",
    "    'pак за pука Гpека цап'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-граммы\n",
    "\n",
    "**N-грамма** — это последовательность из нескольких слов. N указывает на количество элементов и может быть любым. \n",
    "\n",
    "Например, если N равно 1, получаются слова, или **униграммы** (лат. unus, «один»). \n",
    "При N=2 учитываются словосочетания из двух слов — **биграммы** (лат. bis, «дважды»). \n",
    "Если N=3, то это уже **триграммы** (лат. tres, «три»), т.е. из трёх слов.\n",
    "\n",
    "**Задание**\n",
    "Сколько биграмм в тексте **«видеть Гpека в pека pак»** ? \n",
    "А триграмм?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем мешок слов\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# bow, от англ. bag of words\n",
    "bow = count_vect.fit_transform(corpus_greka)\n",
    "\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список уникальных слов в мешке образует **словарь**. Он хранится в счётчике и вызывается методом *get_feature_names()* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Сформируйте мешок слов на корпусе твитов без учета стоп-слов (см. параметры CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Выведите словарь. Каков размер словаря?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизуем твиты с помощью меры TF-IDF\n",
    "\n",
    "> Мешок слов учитывает частоту употребления слов. Посмотрим, как часто уникальное слово встречается во всём корпусе и в отдельном его тексте.\n",
    "\n",
    "Оценка важности слова определяется величиной **TF-IDF** (от англ. *term frequency*, «частота терма, или слова»; *inverse document frequency,* «обратная частота документа, или текста»). \n",
    "\n",
    "То есть *TF* отвечает за количество упоминаний слова в отдельном тексте, а *IDF* отражает частоту его употребления во всём корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TFIDF = TF * IDF$$\n",
    "\n",
    "$$TF = \\frac{t}{n}$$\n",
    "\n",
    "$$IDF = \\log_{10}(\\frac{D}{d})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  Определите TF-IDF для слова *\"рак\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# давайте векторизуем твиты с помощью TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: получите матрицу с векторами TF-IDF для текстового корпуса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучите логистическую регрессию различать позитив и негатив и сделайте оценку метрики accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Проверьте, можно ли улучшить качество классификации с помощью биграмм? \n",
    "# Используйте параметр ngram_range у TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка важности слов\n",
    "\n",
    "> После обучения у каждого слова есть оценка: позитивное оно или негативное.\n",
    "\n",
    "Оценки — это коэффициенты логистической регрессии, которую вы обучили. Если оценка больше нуля, слово положительное. Чем больше коэффициент, тем больше влияние слова на предсказание. Когда оценка меньше нуля, слово отрицательное.\n",
    "\n",
    "Список слов — это имена столбцов в датасете признаков. Значения коэффициентов хранятся в параметре `coef_`  модели.\n",
    "\n",
    "Список слов и список их коэффициентов можно объединить в один массив функцией **zip()** (англ. «застёгивать»), а результат преобразовать в список функцией `list()`. Работает так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['a', 'b', 'c']\n",
    "coefs = [-1, 2, 3]\n",
    "list(zip(words, coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Найдите 10 самых положительных и самых негативных слов по результатам обучения модели. \n",
    "# Это слова с наибольшими коэффициентами регрессии.\n",
    "\n",
    "coefs = zip(features.columns, model.coef_[0])\n",
    "top_10_positive = sorted(coefs, key=lambda x: -x[1])[:10]\n",
    "\n",
    "print(top_10_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дистрибутивная семантика. Word Embeddings. Word2vec\n",
    "\n",
    "![word2vector](https://habrastorage.org/webt/uk/zh/h0/ukzhh0kwiptrhim7tygo1vh5vwq.png)\n",
    "\n",
    "https://habr.com/ru/post/446530/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобученные модели\n",
    "\n",
    "https://rusvectores.org/ru/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим русскоязычную модель с сайта rusvectores\n",
    "# выберем модель **tayga_upos_skipgram_300_2_2019**\n",
    "\n",
    "import zipfile\n",
    "\n",
    "model_file = './data/model_185.zip'\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    wv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# посмотрим словарь модели\n",
    "\n",
    "wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Попробуйте найти синонимы слов с помощью библиотеки gensim и обученного w2v\n",
    "# Сделать это можно с помощью wv.most.similar() (рекомендуем ознакомиться с документацией этого метода)\n",
    "wv.most_similar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Визуализация w2v](https://docs.cortext.net/wp-content/uploads/2016/09/Capture-d’écran-2016-09-13-à-11.20.56-1024x749.png)\n",
    "\n",
    "[Визуализация w2v](https://documents.cortext.net/lib/W2Vexplorer/index.html?config=https://assets.cortext.net/docs/6542799fe444b443e89585491432550a#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Свойства w2v](https://i0.wp.com/migsena.com/wp-content/uploads/2017/09/word2vec.png?zoom=2&w=660)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Воспроизведите взаимосвязь слов по аналогии с рисунком\n",
    "wv.most_similar(positive=['отец_NOUN', 'мужчина_PROPN'], negative=['сын_NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим большой корпус твитов с лемматизацией и токенизацией для word2vec\n",
    "from ast import literal_eval\n",
    "\n",
    "df_tweets = pd.read_csv('./data/tweets_full_tags.csv', sep=';', converters={\"tags\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec выдает вектора для слов, а как быть с вектором текста?\n",
    "# давайте просто возьмем средний вектор всех слов в твите\n",
    "\n",
    "def word_averaging(words, wv):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        if word in wv.vocab:\n",
    "            mean.append(wv.get_vector(word))\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        # print(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(300,)\n",
    "    mean = np.array(mean).mean(axis=0)\n",
    "    return mean, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычислим вектора твитов на всем корпусе\n",
    "\n",
    "df_tweets['vec'] = df_tweets['tags'].apply(word_averaging, args=(wv,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: обучите классификатор тональности твитов на усредненных векторах word2vec (удалите нулевые вектора предварительно)\n",
    "\n",
    "y = df_tweets['positive']\n",
    "X = np.array(df_tweets['vec'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контекстуализированные представления. Революция трансформеров. BERT\n",
    "\n",
    "![Bert](https://www.searchengines.ru/wp-content/uploads/2019/10/1_-oQKmzvHrzqeSQEnM9f_kQ.png)\n",
    "\n",
    "https://habr.com/ru/post/436878/\n",
    "\n",
    "И свежая статья https://m.habr.com/ru/amp/post/487358/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers as ppb # pytorch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем токенайзер для модели BERT, для его инициализации достаточно указать словарь, на котором обучалась предобученная модель\n",
    "# BERT использует собственную токенизацию, никакой предобработки \n",
    "\n",
    "tokenizer = ppb.BertTokenizer(vocab_file='./Rubert/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем текст каждого твита, для BERT не требуется никакая дополнительная предобработка, лемматизация и прочее\n",
    "\n",
    "tokenized = df_tweets['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример токенизации текста: на входе - текст, а на выходе имеем массив с номерами токенов из словаря модели BERT\n",
    "\n",
    "print(df_tweets['text'][0])\n",
    "print(tokenized[0])\n",
    "print(tokenizer.tokenize(df_tweets['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем предобученную модель RuBERT из файла, \n",
    "# в json-файле конфигурации описаны параметры модели\n",
    "\n",
    "config = ppb.BertConfig.from_json_file('./Rubert/bert_config.json')\n",
    "model = ppb.BertModel.from_pretrained('./Rubert/rubert_model.bin', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из-за того, что каждый твит в датасете имеет разную длину (количество токенов)\n",
    "# мы делаем паддинг - заполнение нулями каждого массива токенов до длины максимального массива\n",
    "# чтобы на выходе получить матрицу из токенизированных текстов одной длины\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на размерность матрицы токенизированных твитов после паддинга\n",
    "\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Накладываем маску на значимые токены\n",
    "# В данном случае нам важны все слова кроме нулевых токенов, появившихся на предыдущем шаге паддинга\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Кол-во параметров](https://miro.medium.com/max/4140/1*IFVX74cEe8U5D1GveL1uZA.png)\n",
    "\n",
    "Одни из самых больших NLP моделей и их количество параметров в **милионах**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а теперь сформируем вектора текстов с помощью модели RuBERT\n",
    "\n",
    "# это не быстрый процесс, импортируем инструмент для визуализации времени обработки в цикле\n",
    "from tqdm import notebook\n",
    "\n",
    "# для того, чтобы модель отработала в условиях ограниченных ресурсов - оперативной памяти, мы разделяем входной датасет на батчи.\n",
    "# при батче в 100 твитов потребление оперативной памяти укладывается в 1Гб\n",
    "batch_size = 100\n",
    "\n",
    "# Делаем пустой список для хранения эмбеддингов (векторов) твитов\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        # преобразуем батч с токенизированными твитами в тензор \n",
    "        # по сути тензор - это многомерный массив, который может быть обработан нейронной сетью\n",
    "        input_ids = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        \n",
    "        # создаем тензор и для подготовленной маски\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        # передаем в модель BERT тензор из твитов и маску - на выходе получаем эмбеддинги - вектор текста твита\n",
    "        # torch.no_grad() - для ускорения инференса модели отключим рассчет градиентов\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        # в итоге собираем все эмбеддинги твитов в features\n",
    "        embeddings.append(last_hidden_states[0][:,0,:].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем список батчей эмбеддингов в numpy-матрицу \n",
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводим размерность полученной матрицы эмбеддингов\n",
    "# данная модель BERT формирует вектора текстов в 768-мерном пространстве признаков\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим пример эмбеддинга для твита - осторожно много цифр!\n",
    "\n",
    "print(df_tweets['text'][0])\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки для обучения классификатора на logreg и оценки качества\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним целевую переменную: метку тональности позитив/негатив\n",
    "\n",
    "labels = df_tweets['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем матрицу признаков и целевую переменную на обучающий и тестовый набор\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем классификатор на основе логистической регрессии\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем пробное предсказание\n",
    "tweet_index = 555\n",
    "\n",
    "print('Text: ' + df_tweets['text'][tweet_index])\n",
    "print('Predict label: ', lr_clf.predict(features[tweet_index:tweet_index+1][:])[0])\n",
    "print('True label: ', df_tweets['positive'][tweet_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оцениваем accuracy на тестовой выборке\n",
    "\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}